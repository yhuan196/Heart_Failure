---
title: "Validation"
author: "Xuesen Zhao"
date: "2023-11-28"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Methods
### ROC Curves and AUC
In our analysis, we employed time-dependent Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) values to evaluate the discriminative ability of our Cox proportional hazards model over time. Specifically, we focused on two clinically relevant time points: 50 days and 250 days. The ROC curve is a graphical representation that illustrates the diagnostic ability of a binary classifier system by plotting the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. AUC, a key summary measure of the ROC curve, quantifies the overall ability of the model to discriminate between individuals who will experience the event and those who will not, irrespective of the chosen probability threshold. Higher AUC values indicate better discriminative ability.

### C-Index
The concordance index (C-index) was calculated to assess the predictive accuracy of our model. This metric is a measure of the model's ability to correctly rank the survival times of pairs of individuals, considering censored data. The C-index is calculated through pairwise comparisons, where a pair is concordant if the individual predicted to have a shorter survival time indeed experiences the event earlier than the other individual in the pair. A C-index of 0.5 suggests no better predictive accuracy than random chance, while a value of 1 indicates perfect prediction.

### Calibration Metrics
To evaluate the calibration of our model, we focused on two metrics: calibration slope and calibration-in-the-large. Calibration reflects the agreement between observed outcomes and predicted probabilities. The calibration slope assesses whether the predicted risks are of the correct magnitude. A slope of 1 indicates perfect calibration, meaning the model's predicted probabilities are accurately scaled. Calibration-in-the-large measures systematic bias in predictions, where a value close to 0 implies no bias. These metrics were calculated using logistic regression within a bootstrap framework to capture the variability in model predictions and provide robust estimates.

## ROC curve

```{r}
# read data
data <- read_csv("data/heart_failure.csv")

# # data cleaning
data <- data |>
  arrange(TIME) |>
  janitor::clean_names() |>
  mutate(gender = factor(gender),
         smoking = factor(smoking),
         diabetes = factor(diabetes),
         bp = factor(bp),
         # event = factor(event),
         anaemia = factor(anaemia)) |>
  rename(pletelets = pletelets)|>
  mutate(ef_cat = factor(case_when(
    ejection_fraction <= 30 ~ "Low",
    ejection_fraction > 30 & ejection_fraction < 45 ~ "Medium",
    ejection_fraction >= 45 ~ "High"
  )))
```


```{r}
# Load necessary libraries
library(survival)
library(boot)
library(rms)
library(survivalROC)
library(ggplot2)
library(timeROC)
library(rms)


# Cox model
step_model <- coxph(Surv(time, event) ~ log(creatinine+1) + age + ef_cat + bp +
                      sodium, data = model_data)


# Calculate predicted risks
predicted_risks <- predict(step_model, newdata = model_data, type = "risk")

# Time points for ROC analysis
time_points <- c(50, 250)

# Calculate ROC curves at specified times
roc_50 <- timeROC(T = model_data$time, delta = model_data$event, marker = predicted_risks, times = 50, cause = 1)
roc_250 <- timeROC(T = model_data$time, delta = model_data$event, marker = predicted_risks, times = 250, cause = 1)

# Extract AUC values
auc_50 <- roc_50$AUC
auc_250 <- roc_250$AUC

# Print AUC values
print(paste("AUC at 50 days:", auc_50))
print(paste("AUC at 250 days:", auc_250))

# Plot ROC curves
plot(roc_50$FP, roc_50$TP, type = "l", col = "red", xlab = "1 - Specificity", ylab = "Sensitivity", main = "Time-Dependent ROC Curves")
lines(roc_250$FP, roc_250$TP, type = "l", col = "blue")
legend("bottomright", legend = c("50 days", "250 days"), col = c("red", "blue"), lty = 1)
abline(0, 1, col = "black", lty = 2)
```


## C-statistics 

```{r}
# Define the function for calculating C-statistic
boot_c_statistic <- function(original_data, indices) {
  # Creating a bootstrap sample
  boot_data <- original_data[indices, ]

  # Fit the Cox model to the bootstrap sample
  fit <-   coxph(Surv(time, event) ~ log(creatinine+1) + age + ef_cat + bp +
                      sodium, data = boot_data)
  
  # Calculate the concordance statistic using the updated function
  concordance <- concordance(fit)$concordance
  return(concordance)
}

# Perform bootstrapping for C-statistic
set.seed(123) # for reproducibility
boot_results_c_stat <- boot(data = model_data, statistic = boot_c_statistic, R = 400)

# Calculate the average C-statistic
mean_c_stat <- mean(boot_results_c_stat$t)
print(mean_c_stat)
```


## Calibration slope

```{r}
# Define the bootstrap function for calibration metrics using logistic regression
boot_calibration_logistic <- function(original_data, indices) {
  boot_data <- original_data[indices, ]
  fit <-  coxph(Surv(time, event) ~ log(creatinine+1) + age + ef_cat + bp +
                      sodium, data = boot_data)
  
  # Predicted risks for the original dataset
  predicted_risks <- predict(fit, newdata = original_data, type = "risk")
  
  # Fit a logistic model for calibration
  calibration_model_logistic <- glm(event ~ predicted_risks, data = original_data, family = "binomial")
  
  # Calibration-in-the-large (intercept) and calibration slope (coefficient of predicted_risks)
  calibration_in_the_large <- coef(calibration_model_logistic)[1]
  calibration_slope_logistic <- coef(calibration_model_logistic)["predicted_risks"]
  
  return(c(calibration_in_the_large, calibration_slope_logistic))
}

# Perform bootstrap
set.seed(123)
boot_results_logistic <- boot(data = model_data, statistic = boot_calibration_logistic, R = 400)

# Calculate the average calibration-in-the-large and calibration slope
mean_calibration_in_the_large <- mean(boot_results_logistic$t[, 1])
mean_calibration_slope_logistic <- mean(boot_results_logistic$t[, 2])

print(paste("Mean Calibration-in-the-Large:", mean_calibration_in_the_large))
print(paste("Mean Calibration Slope:", mean_calibration_slope_logistic))

```
```{r}
# Define the bootstrap function for calibration metrics using logistic regression
boot_calibration_logistic <- function(original_data, indices) {
  boot_data <- original_data[indices, ]
  fit <-  coxph(Surv(time, event) ~ log(creatinine+1) + age + ef_cat + bp +
                      sodium, data = boot_data)
  
  
  # Predicted risks for the original dataset
  predicted_risks <- predict(fit, newdata = original_data, type = "risk")
  
  # Fit a logistic model for calibration
  calibration_model_logistic <- glm(event ~ predicted_risks, data = original_data, family = "binomial")
  
  # Calibration slope (coefficient of predicted_risks)
  calibration_slope_logistic <- coef(calibration_model_logistic)["predicted_risks"]
  
  return(calibration_slope_logistic)
}

# Perform bootstrap
set.seed(123)
boot_results_logistic <- boot(data = model_data, statistic = boot_calibration_logistic, R = 400)

# Calculate the average calibration slope
mean_calibration_slope_logistic <- mean(boot_results_logistic$t)
print(mean_calibration_slope_logistic)

```

## Results
### ROC Curves and AUC
Our time-dependent ROC analysis at 50 days yielded an AUC of approximately 0.74, while at 250 days, the AUC was around 0.94. These values indicate that the model's ability to discriminate between those who will experience the event and those who will not improves over time. The ROC curves further visually demonstrate this improvement, with the curve for 250 days being closer to the top left corner, indicating better performance.

### C-Index
The average C-index calculated through bootstrapping (n = 400) was about 0.75. This suggests that in 75% of pairwise comparisons, our model correctly ranks the survival times. A C-index of 0.75 is generally indicative of good predictive ability, especially in clinical settings where accurate risk stratification is crucial for treatment planning.

### Calibration Metrics
The mean calibration slope obtained was approximately 0.371, indicating that the model might be underestimating risks. This value suggests that while the model is effective in ranking individuals by risk, it may not provide accurate probability estimates for the occurrence of events. This discrepancy points to a potential need for model refinement to improve calibration.



## Discussion

The findings from our analysis present a nuanced picture of the model's performance. While the model demonstrates strong discriminative ability (as evidenced by the AUC values and C-index), the calibration assessment reveals areas for improvement. The low calibration slope, in particular, suggests that the model's predicted probabilities may not align well with the observed frequencies, potentially leading to conservative risk estimates. This underestimation of risk could have significant implications in clinical settings, where over- or underestimation of risk can impact treatment decisions.

The improvement in discriminative ability over time, as indicated by the increasing AUC values from 50 to 250 days, might reflect the evolving nature of risk factors and their impact on patient outcomes over the course of the disease. However, the calibration results highlight the importance of not solely relying on discriminative metrics but also considering how well the predicted probabilities reflect reality.

In light of these findings, future work may involve revisiting the model's complexity and variable selection, exploring alternative modeling approaches, or applying more advanced calibration techniques. External validation on a separate dataset could also provide further insights into the model's generalizability and real-world applicability.
